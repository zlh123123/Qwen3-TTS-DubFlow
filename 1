import json
import os
import re
from typing import List, Dict, Any
from tqdm import tqdm
from vllm import LLM, SamplingParams
from difflib import SequenceMatcher

# ================= é…ç½®åŒºåŸŸ =================
# æ¨¡å‹è·¯å¾„ (ä¿æŒä½ æœ¬åœ°çš„é…ç½®)
MODEL_PATH = "/mnt/tenant-home_speed/shanghai/zlh/recover_biaodian/qwen3_gen/model"

# è¾“å…¥è¾“å‡ºæ–‡ä»¶
NOVEL_PATH = "novel.txt"
CHARACTERS_PATH = "characters.json"
OUTPUT_FILE = "script_lines.json"
TEMP_JSONL = "raw_script_temp.jsonl" 

# åˆ‡ç‰‡é…ç½®
CHUNK_SIZE = 3000       # æ¯ä¸ªåˆ‡ç‰‡å¤„ç†çš„å­—ç¬¦æ•°
OVERLAP_SIZE = 500      # ä¸Šä¸‹æ–‡é‡å å¤§å°
GPU_MEMORY_UTILIZATION = 0.8
TENSOR_PARALLEL_SIZE = 2

# ================= æç¤ºè¯æ¨¡æ¿ (å·²ä¿®å¤ KeyError) =================
# æ³¨æ„ï¼šä¸‹æ–¹çš„ JSON ç¤ºä¾‹ä½¿ç”¨äº†åŒèŠ±æ‹¬å· {{ }} æ¥è½¬ä¹‰ï¼Œé˜²æ­¢ .format() æŠ¥é”™
SYSTEM_PROMPT_TEMPLATE = """
# Role
ä½ æ˜¯ä¸€ä¸ªå‰§æœ¬æ”¹ç¼–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†å°è¯´æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„åŒ–çš„å‰§æœ¬æ ¼å¼ã€‚

# Known Characters
ä»¥ä¸‹æ˜¯æœ¬å°è¯´çš„ä¸»è¦è§’è‰²åˆ—è¡¨ï¼Œè¯·ä¸¥æ ¼ä»è¿™é‡Œé€‰æ‹© `speaker_id`ï¼š
{characters_json}

# Rules
1. å°†æ–‡æœ¬æ‹†è§£ä¸ºã€å¯¹è¯ã€‘(dialogue) å’Œã€æ—ç™½ã€‘(narration)ã€‚
2. å¯¹äºã€å¯¹è¯ã€‘ï¼š
   - **æœ€é‡è¦çš„è§„åˆ™ï¼šå¿…é¡»ç»“åˆã€ä¸Šä¸‹æ–‡ã€‘å°¤å…¶æ˜¯ã€å¯¹è¯åçš„æå†™ã€‘æ¥åˆ¤æ–­è¯´è¯äººã€‚**
   - è­¦æƒ•ï¼šä¸è¦é»˜è®¤å¯¹è¯å±äºä¸Šä¸€å¥çš„ä¸»è¯­ï¼å¦‚æœå¯¹è¯åæè¿°äº†â€œæŸäººè½¬å¤´çœ‹â€ã€â€œæŸäººèµ°äº†å‡ºæ¥â€ï¼Œè¯´æ˜è¯´è¯è€…æ˜¯æ–°å‡ºç°çš„äººã€‚
   - å¿…é¡»æŒ‡æ˜ `speaker_id`ã€‚
   - ç”Ÿæˆ `instruction` (30å­—ä»¥å†…çš„è‡ªç„¶è¯­è¨€å£°éŸ³æ¼”å‡ºæç¤º)ã€‚
3. å¯¹äºã€æ—ç™½ã€‘ï¼š
   - `speaker_id` è®¾ä¸º "narrator"ã€‚
4. é‡åˆ°æœªçŸ¥è·¯äººï¼Œ`speaker_id` è®¾ä¸º "unknown"ã€‚

# Few-Shot Examples (æ€ç»´é“¾ç¤ºä¾‹)
ä¸ºäº†ä¿è¯å‡†ç¡®ç‡ï¼Œè¯·å‚è€ƒä»¥ä¸‹æ¨ç†é€»è¾‘ï¼š

ã€åŸæ–‡ã€‘
é™ˆé»˜çš±äº†çš±çœ‰ï¼Œæ²¡æœ‰ç«‹åˆ»ççœ¼ã€‚
â€œæ“ï¼è¿™ä»–å¦ˆæ˜¯å“ªå„¿ï¼Ÿï¼â€
é™ˆé»˜è½¬å¤´çœ‹å‘å£°éŸ³æ¥æºã€‚è§’è½é‡Œï¼Œä¸€ä¸ªå£®æ±‰æ‘‡æ‘‡æ™ƒæ™ƒåœ°ç«™äº†èµ·æ¥ã€‚

ã€é”™è¯¯ç¤ºèŒƒã€‘(åªçœ‹ä¸Šæ–‡)
{{"type": "dialogue", "speaker_id": "chen_mo", ...}} -> é”™è¯¯ï¼è™½ç„¶å‰æ–‡æ˜¯é™ˆé»˜ï¼Œä½†è¿™å¥å¼å«å¯¼è‡´é™ˆé»˜å»â€œçœ‹â€åˆ«äººï¼Œæ‰€ä»¥ä¸æ˜¯é™ˆé»˜è¯´çš„ã€‚

ã€æ­£ç¡®ç¤ºèŒƒã€‘(ç»“åˆåæ–‡æ¨ç†)
{{"type": "dialogue", "speaker_id": "lei_hu", "text": "æ“ï¼è¿™ä»–å¦ˆæ˜¯å“ªå„¿ï¼Ÿï¼", "instruction": "ç‚¸é›·èˆ¬çš„æ€’å¼ï¼Œè¯­é€Ÿæ€¥ä¿ƒï¼Œæƒ…ç»ªå¤„äºçˆ†å‘è¾¹ç¼˜"}}

# Output Format (JSON Lines)
è¯·ç›´æ¥è¾“å‡º JSON Linesã€‚
{{"type": "narration", "speaker_id": "narrator", "text": "å¤©ç©ºä¸‹èµ·äº†å¤§é›¨ã€‚", "instruction": "å‹æŠ‘çš„æ°›å›´"}}
{{"type": "dialogue", "speaker_id": "li_yunlong", "text": "äºŒè¥é•¿ï¼ä½ çš„ç‚®å‘¢ï¼Ÿ", "instruction": "æåº¦æ„¤æ€’çš„å˜¶å¼"}}
"""

# ================= è¾…åŠ©å‡½æ•° =================

def load_text(path):
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

def load_characters(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è§’è‰²æ–‡ä»¶: {path}ï¼Œè¯·å…ˆè¿è¡Œ extract_characters.py")
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_chunks(text, chunk_size, overlap):
    """
    ç”Ÿæˆæ»‘åŠ¨çª—å£åˆ‡ç‰‡
    """
    chunks = []
    start = 0
    text_len = len(text)
    
    while start < text_len:
        end = min(start + chunk_size, text_len)
        # ä¸Šä¸‹æ–‡å›æº¯
        context_start = max(0, start - overlap) 
        chunk_text = text[context_start:end]
        
        chunks.append({
            "global_start": start,
            "context_start": context_start,
            "text": chunk_text
        })
        
        start += chunk_size 
        
    return chunks

def clean_llm_output(text):
    """æå– JSONL è¡Œï¼Œå¢åŠ å¯¹ instruction çš„å…¼å®¹å¤„ç†"""
    lines = text.strip().split('\n')
    valid_objs = []
    for line in lines:
        line = line.strip()
        # ç§»é™¤ Markdown æ ‡è®°
        line = re.sub(r"^```json", "", line)
        line = re.sub(r"^```", "", line)
        if not line: continue
        
        try:
            obj = json.loads(line)
            # åŸºç¡€æ ¡éªŒ
            if "text" in obj and "type" in obj:
                # å¦‚æœæ¨¡å‹æ¼å†™äº† instructionï¼Œç»™ä¸ªé»˜è®¤å€¼é˜²æ­¢æŠ¥é”™
                if "instruction" not in obj:
                    obj["instruction"] = "å¹³é™è‡ªç„¶çš„è¯­æ°”"
                
                # æˆªæ–­è¿‡é•¿çš„ instruction (è™½ç„¶ Prompt é™åˆ¶äº†ï¼Œä½†ä¸ºäº†ä¿é™©)
                if len(obj["instruction"]) > 50:
                    obj["instruction"] = obj["instruction"][:50]
                
                valid_objs.append(obj)
        except:
            continue
    return valid_objs

def is_similar(s1, s2, threshold=0.8):
    """åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸ä¼¼ï¼ˆç”¨äºå»é‡ï¼‰"""
    return SequenceMatcher(None, s1, s2).ratio() > threshold

def merge_results(all_chunks_results):
    """
    åˆå¹¶é€»è¾‘ï¼šå¤„ç†é‡å åŒºåŸŸ
    """
    final_lines = []
    print("ğŸ”„ æ­£åœ¨åˆå¹¶åˆ‡ç‰‡ç»“æœå¹¶å»é‡...")
    
    for chunk_res in all_chunks_results:
        for item in chunk_res:
            curr_text = item.get("text", "")
            if not curr_text: continue
            
            # ç®€å•çš„æµå¼å»é‡ï¼šæ£€æŸ¥æ˜¯å¦ä¸ä¸Šä¸€å¥é«˜åº¦é‡å¤
            if final_lines:
                prev_text = final_lines[-1].get("text", "")
                if is_similar(curr_text, prev_text, threshold=0.9):
                    continue
            
            final_lines.append(item)
            
    return final_lines

# ================= ä¸»ç¨‹åº =================

def main():
    # 1. åŠ è½½æ•°æ®
    print("ğŸ“– åŠ è½½å°è¯´å’Œè§’è‰²åº“...")
    if not os.path.exists(NOVEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {NOVEL_PATH}")
        return

    full_text = load_text(NOVEL_PATH)
    characters = load_characters(CHARACTERS_PATH)
    
    # ç®€åŒ–è§’è‰²åˆ—è¡¨ç”¨äº Prompt (å‡å°‘ token æ¶ˆè€—)
    char_summary = []
    for c in characters:
        char_summary.append({
            "id": c.get("id"),
            "name": c.get("name"),
            "gender": c.get("gender"),
            "personality": c.get("personality_tags", [])
        })
    char_json_str = json.dumps(char_summary, ensure_ascii=False, indent=2)

    # 2. åˆå§‹åŒ–æ¨¡å‹
    print("ğŸš€ åŠ è½½ vLLM æ¨¡å‹...")
    llm = LLM(
        model=MODEL_PATH,
        dtype="auto",
        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,
        tensor_parallel_size=TENSOR_PARALLEL_SIZE,
        trust_remote_code=True,
        max_model_len=32768,
        enforce_eager=True
    )
    tokenizer = llm.get_tokenizer()

    # 3. åˆ‡ç‰‡
    chunks = create_chunks(full_text, CHUNK_SIZE, OVERLAP_SIZE)
    print(f"ğŸ”ª æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")

    # 4. æ‰¹é‡æ„é€  Prompt
    prompts = []
    # è°ƒæ•´é‡‡æ ·å‚æ•°ï¼Œtemperature ç¨å¾®è°ƒé«˜ä¸€ç‚¹ç‚¹å¯ä»¥è®© instruction æ›´è‡ªç„¶ï¼Œä½†ä¸è¦å¤ªé«˜
    sampling_params = SamplingParams(
        temperature=0.3, 
        top_p=0.9,
        max_tokens=4096,
        stop=["<|endoftext|>", "<|im_end|>"]
    )
    
    # è¿™é‡Œä¸ä¼šå†æŠ¥é”™äº†
    system_prompt_filled = SYSTEM_PROMPT_TEMPLATE.format(characters_json=char_json_str)

    print("ğŸ“ æ„é€  Prompts...")
    for chunk in chunks:
        user_content = f"ã€å½“å‰æ–‡æœ¬ç‰‡æ®µã€‘:\n{chunk['text']}"
        
        messages = [
            {"role": "system", "content": system_prompt_filled},
            {"role": "user", "content": user_content}
        ]
        
        # ä½¿ç”¨ apply_chat_template ä¸” tokenize=Falseï¼Œè¿”å›å­—ç¬¦ä¸²
        text_prompt = tokenizer.apply_chat_template(
            messages, 
            add_generation_prompt=True, 
            tokenize=False
        )
        prompts.append(text_prompt)

    # 5. æ‰¹é‡æ¨ç†
    print(f"ğŸ§  å¼€å§‹æ¨ç† (Batch Size = {len(prompts)})...")
    outputs = llm.generate(prompts, sampling_params)

    # 6. å¤„ç†ç»“æœ
    all_raw_results = []
    
    # æ¸…ç©ºä¸´æ—¶æ–‡ä»¶
    with open(TEMP_JSONL, 'w', encoding='utf-8') as f:
        pass

    for i, output in enumerate(tqdm(outputs, desc="è§£æè¿›åº¦")):
        generated_text = output.outputs[0].text
        
        # è§£æ
        parsed_objs = clean_llm_output(generated_text)
        all_raw_results.append(parsed_objs)
        
        # å¤‡ä»½
        with open(TEMP_JSONL, 'a', encoding='utf-8') as f:
            for obj in parsed_objs:
                f.write(json.dumps(obj, ensure_ascii=False) + "\n")

    # 7. åˆå¹¶
    final_script = merge_results(all_raw_results)
    
    # 8. ä¿å­˜
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_script, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š å…±æå–å°è¯/æ—ç™½: {len(final_script)} æ¡")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_FILE)}")
    
    # æ‰“å°é¢„è§ˆï¼Œæ£€æŸ¥ instruction æ˜¯å¦ç¬¦åˆè¦æ±‚
    if final_script:
        print("\nğŸ” ç»“æœé¢„è§ˆ (Instruction æ£€æŸ¥):")
        for item in final_script[:3]:
            print(f"è§’è‰²: {item.get('speaker_id')}")
            print(f"å°è¯: {item.get('text')[:20]}...")
            print(f"æç¤º: {item.get('instruction')}")
            print("-" * 30)

if __name__ == "__main__":
    if not os.path.exists(NOVEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {NOVEL_PATH}")
    elif not os.path.exists(CHARACTERS_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {CHARACTERS_PATH}")
    else:
        main()
